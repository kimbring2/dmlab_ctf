{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-1-d33ea1ffe2fe>\", line 366, in train_threading\n",
      "  File \"/home/kimbring2/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 885, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/kimbring2/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 933, in _call\n",
      "    self._initialize(args, kwds, add_initializers_to=initializers)\n",
      "  File \"/home/kimbring2/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 759, in _initialize\n",
      "    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n",
      "  File \"/home/kimbring2/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3066, in _get_concrete_function_internal_garbage_collected\n",
      "    graph_function, _ = self._maybe_define_function(args, kwargs)\n",
      "  File \"/home/kimbring2/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3463, in _maybe_define_function\n",
      "    graph_function = self._create_graph_function(args, kwargs)\n",
      "  File \"/home/kimbring2/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3298, in _create_graph_function\n",
      "    func_graph_module.func_graph_from_py_func(\n",
      "  File \"/home/kimbring2/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 1007, in func_graph_from_py_func\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\n",
      "  File \"/home/kimbring2/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 668, in wrapped_fn\n",
      "    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "  File \"/home/kimbring2/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3990, in bound_method_wrapper\n",
      "    return wrapped_fn(*args, **kwargs)\n",
      "  File \"/home/kimbring2/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 994, in wrapper\n",
      "    raise e.ag_error_metadata.to_exception(e)\n",
      "ValueError: in user code:\n",
      "\n",
      "    <ipython-input-1-d33ea1ffe2fe>:152 act_old  *\n",
      "        prediction = self.policy_old(state, memory_state, carry_state, training=training)\n",
      "    <ipython-input-1-d33ea1ffe2fe>:53 call  *\n",
      "        conv_2_reshaped = self.reshape(conv_2_flattend)\n",
      "    /home/kimbring2/.local/lib/python3.8/site-packages/keras/engine/base_layer.py:1037 __call__  **\n",
      "        outputs = call_fn(inputs, *args, **kwargs)\n",
      "    /home/kimbring2/.local/lib/python3.8/site-packages/keras/layers/core.py:534 call\n",
      "        result = tf.reshape(\n",
      "    /home/kimbring2/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n",
      "        return target(*args, **kwargs)\n",
      "    /home/kimbring2/.local/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:196 reshape\n",
      "        result = gen_array_ops.reshape(tensor, shape, name)\n",
      "    /home/kimbring2/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:8403 reshape\n",
      "        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
      "    /home/kimbring2/.local/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py:748 _apply_op_helper\n",
      "        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n",
      "    /home/kimbring2/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:599 _create_op_internal\n",
      "        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n",
      "    /home/kimbring2/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:3561 _create_op_internal\n",
      "        ret = Operation(\n",
      "    /home/kimbring2/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:2041 __init__\n",
      "        self._c_op = _create_c_op(self._graph, node_def, inputs,\n",
      "    /home/kimbring2/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1883 _create_c_op\n",
      "        raise ValueError(str(e))\n",
      "\n",
      "    ValueError: Cannot reshape a tensor with 1800 elements to shape [1,266,8] (2128 elements) for '{{node actor_critic_1/reshape_1/Reshape}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](actor_critic_1/flatten_1/Reshape, actor_critic_1/reshape_1/Reshape/shape)' with input shapes: [1,1800], [3] and with input tensors computed as partial shapes: input[1] = [1,266,8].\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d33ea1ffe2fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0;31m#agent.run() # use as A2C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# use as A3C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m     \u001b[0;31m#agent.test('Models/Pong-v0_A3C_2.5e-05_Actor.h5', '')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-d33ea1ffe2fe>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_threads)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m             \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Tutorial by www.pylessons.com\n",
    "# Tutorial written for - Tensorflow 2.3.1\n",
    "\n",
    "import os\n",
    "import random\n",
    "import gym\n",
    "import pylab\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Add, Conv2D, Flatten, LSTM, Reshape\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras import backend as K\n",
    "import cv2\n",
    "import threading\n",
    "from threading import Thread, Lock\n",
    "import time\n",
    "import tensorflow_probability as tfp\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "import deepmind_lab\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_virtual_device_configuration(gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4000)])\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1' # -1:cpu, 0:first gpu\n",
    "\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "\n",
    "class ActorCritic(tf.keras.Model):\n",
    "    def __init__(self, input_shape, action_space):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.conv_1 = Conv2D(4, 3, 2, padding=\"valid\", activation=\"relu\")\n",
    "        self.conv_2 = Conv2D(8, 3, 2, padding=\"valid\", activation=\"relu\")\n",
    "        \n",
    "        self.lstm = LSTM(128, name=\"lstm\", return_sequences=True, return_state=True)\n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        self.reshape = Reshape((15*15,8))\n",
    "        self.dense_0 = Dense(512, activation='relu')\n",
    "        self.dense_1 = Dense(action_space)\n",
    "        self.dense_2 = Dense(1)\n",
    "        \n",
    "    def call(self, X_input, memory_state, carry_state):\n",
    "        conv_1 = self.conv_1(X_input)\n",
    "        conv_2 = self.conv_2(conv_1)\n",
    "        #print(\"conv_2.shape: \", conv_2.shape)\n",
    "        \n",
    "        conv_2_flattend = self.flatten(conv_2)\n",
    "        conv_2_reshaped = self.reshape(conv_2_flattend)\n",
    "        \n",
    "        initial_state = (memory_state, carry_state)\n",
    "        lstm_output, next_memory_state, next_carry_state = self.lstm(conv_2_reshaped, initial_state)\n",
    "        \n",
    "        X_input = self.flatten(lstm_output)\n",
    "        X_input = self.dense_0(X_input)\n",
    "\n",
    "        action_logit = self.dense_1(X_input)\n",
    "        value = self.dense_2(X_input)\n",
    "        \n",
    "        return action_logit, value, next_memory_state, next_carry_state\n",
    "\n",
    "\n",
    "def safe_log(x):\n",
    "  \"\"\"Computes a safe logarithm which returns 0 if x is zero.\"\"\"\n",
    "  return tf.where(\n",
    "      tf.math.equal(x, 0),\n",
    "      tf.zeros_like(x),\n",
    "      tf.math.log(tf.math.maximum(1e-12, x)))\n",
    "\n",
    "\n",
    "def take_vector_elements(vectors, indices):\n",
    "    \"\"\"\n",
    "    For a batch of vectors, take a single vector component\n",
    "    out of each vector.\n",
    "    Args:\n",
    "      vectors: a [batch x dims] Tensor.\n",
    "      indices: an int32 Tensor with `batch` entries.\n",
    "    Returns:\n",
    "      A Tensor with `batch` entries, one for each vector.\n",
    "    \"\"\"\n",
    "    return tf.gather_nd(vectors, tf.stack([tf.range(tf.shape(vectors)[0]), indices], axis=1))\n",
    "\n",
    "\n",
    "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
    "sparse_ce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM)\n",
    "mse_loss = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "workspace_path = '/home/kimbring2/stabilizing_transformers'\n",
    "writer = tf.summary.create_file_writer(workspace_path + \"/tensorboard/pong\")\n",
    "\n",
    "def _action(*entries):\n",
    "  return np.array(entries, dtype=np.intc)\n",
    "\n",
    "ACTIONS = {\n",
    "      'look_left': _action(-20, 0, 0, 0, 0, 0, 0),\n",
    "      'look_right': _action(20, 0, 0, 0, 0, 0, 0),\n",
    "      'look_up': _action(0, 10, 0, 0, 0, 0, 0),\n",
    "      'look_down': _action(0, -10, 0, 0, 0, 0, 0),\n",
    "      'strafe_left': _action(0, 0, -1, 0, 0, 0, 0),\n",
    "      'strafe_right': _action(0, 0, 1, 0, 0, 0, 0),\n",
    "      'forward': _action(0, 0, 0, 1, 0, 0, 0),\n",
    "      'backward': _action(0, 0, 0, -1, 0, 0, 0),\n",
    "      'fire': _action(0, 0, 0, 0, 1, 0, 0),\n",
    "      'jump': _action(0, 0, 0, 0, 0, 1, 0),\n",
    "      'crouch': _action(0, 0, 0, 0, 0, 0, 1)\n",
    "  }\n",
    "\n",
    "ACTION_LIST = list(ACTIONS)\n",
    "\n",
    "class A3CAgent:\n",
    "    # Actor-Critic Main Optimization Algorithm\n",
    "    def __init__(self, env_name):\n",
    "        # Initialization\n",
    "        # Environment and PPO parameters\n",
    "        self.env_name = env_name       \n",
    "        self.env = deepmind_lab.Lab(self.env_name, ['RGB_INTERLEAVED'],\n",
    "                       {'fps': '15', 'width': '64', 'height': '64'})\n",
    "        #env.reset(seed=1)\n",
    "        \n",
    "        self.action_size = len(ACTION_LIST)\n",
    "        self.EPISODES, self.episode, self.max_average = 20000, 0, -21.0 # specific for pong\n",
    "        self.lock = Lock()\n",
    "        self.learning_rate = 0.0001\n",
    "\n",
    "        self.ROWS = 64\n",
    "        self.COLS = 64\n",
    "        self.REM_STEP = 3\n",
    "\n",
    "        # Instantiate plot memory\n",
    "        self.scores, self.episodes, self.average = [], [], []\n",
    "\n",
    "        self.Save_Path = 'Models'\n",
    "        self.state_size = (self.REM_STEP, self.ROWS, self.COLS)\n",
    "        \n",
    "        if not os.path.exists(self.Save_Path): os.makedirs(self.Save_Path)\n",
    "        self.path = '{}_A3C_{}'.format(self.env_name, self.learning_rate)\n",
    "        self.model_name = os.path.join(self.Save_Path, self.path)\n",
    "\n",
    "        # Create Actor-Critic network model\n",
    "        self.policy = ActorCritic(input_shape=self.state_size, action_space=self.action_size)\n",
    "        self.policy_old = ActorCritic(input_shape=self.state_size, action_space=self.action_size)\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam(self.learning_rate)\n",
    "\n",
    "    @tf.function\n",
    "    def act_old(self, state, memory_state, carry_state, training):\n",
    "        # Use the network to predict the next action to take, using the model\n",
    "        prediction = self.policy_old(state, memory_state, carry_state, training=training)\n",
    "        action = tf.random.categorical(prediction[0], 1)\n",
    "        \n",
    "        return action[0][0], prediction\n",
    "    \n",
    "    @tf.function\n",
    "    def act_new(self, state, memory_state, carry_state, training):\n",
    "        # Use the network to predict the next action to take, using the model\n",
    "        prediction = self.policy(state, memory_state, carry_state, training=training)\n",
    "        action = tf.random.categorical(prediction[0], 1)\n",
    "        \n",
    "        return action[0][0], prediction\n",
    "\n",
    "    def discount_rewards(self, reward):\n",
    "        # Compute the gamma-discounted rewards over an episode\n",
    "        gamma = 0.99    # discount rate\n",
    "        running_add = 0\n",
    "        discounted_r = np.zeros_like(reward)\n",
    "        for i in reversed(range(0, len(reward))):\n",
    "            if reward[i] != 0: # reset the sum, since this was a game boundary (pong specific!)\n",
    "                running_add = 0\n",
    "\n",
    "            running_add = running_add * gamma + reward[i]\n",
    "            discounted_r[i] = running_add\n",
    "\n",
    "        if np.std(discounted_r) != 0:\n",
    "            discounted_r -= np.mean(discounted_r) # normalizing the result\n",
    "            discounted_r /= np.std(discounted_r) # divide by standard deviation\n",
    "\n",
    "        return discounted_r\n",
    "    \n",
    "    def get_loss(self, states, actions, discounted_r, initial_memory_state, initial_carry_state):\n",
    "        batch_size = states.shape[0]\n",
    "        \n",
    "        #states = np.vstack(states)\n",
    "        \n",
    "        action_logits_old = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "        values_old = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "        \n",
    "        action_logits = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "        values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "        \n",
    "        memory_state_old = initial_memory_state\n",
    "        carry_state_old = initial_carry_state\n",
    "        \n",
    "        memory_state = initial_memory_state\n",
    "        carry_state = initial_carry_state\n",
    "        for i in range(0, batch_size):\n",
    "            #print(\"states[i].shape: \", states[i].shape)\n",
    "            #prediction_old = self.policy_old(tf.expand_dims(states[i,:], 0), training=True)\n",
    "            action_old, prediction_old = agent.act_old(tf.expand_dims(states[i,:], 0), \n",
    "                                                       memory_state_old, carry_state_old,\n",
    "                                                       training=True)\n",
    "            action_logit_old = prediction_old[0]\n",
    "            value_old = prediction_old[1]\n",
    "            memory_state_old = prediction_old[2]\n",
    "            carry_state_old = prediction_old[3]\n",
    "\n",
    "            action_logits_old = action_logits_old.write(i, action_logit_old[0])\n",
    "            #print(\"tf.squeeze(value_old): \", tf.squeeze(value_old))\n",
    "            values_old = values_old.write(i,  tf.squeeze(value_old))\n",
    "            \n",
    "            #prediction = self.policy(tf.expand_dims(states[i,:], 0), training=True)\n",
    "            action, prediction = agent.act_new(tf.expand_dims(states[i,:], 0), \n",
    "                                               memory_state, carry_state,\n",
    "                                               training=True)\n",
    "            action_logit = prediction[0]\n",
    "            value = prediction[1]\n",
    "            memory_state = prediction[2]\n",
    "            carry_state = prediction[3]\n",
    "            \n",
    "            action_logits = action_logits.write(i, action_logit[0])\n",
    "            values = values.write(i,  tf.squeeze(value))\n",
    "            \n",
    "        action_logits_old = action_logits_old.stack()\n",
    "        values_old = values_old.stack()\n",
    "        action_logits = action_logits.stack()\n",
    "        values = values.stack()\n",
    "        \n",
    "        action_logits_selected_old = take_vector_elements(action_logits_old, actions)\n",
    "        \n",
    "        action_logits_selected = take_vector_elements(action_logits, actions)\n",
    "        \n",
    "        advantages = discounted_r - tf.stop_gradient(values_old)\n",
    "            \n",
    "        entropy_loss_old = tf.keras.losses.categorical_crossentropy(action_logits_old, action_logits_old)\n",
    "            \n",
    "        action_logits_selected_old = tf.nn.softmax(action_logits_selected_old)\n",
    "        action_log_selected_old = tf.math.log(action_logits_selected_old)\n",
    "        \n",
    "        action_logits_selected = tf.nn.softmax(action_logits_selected)\n",
    "        action_log_selected = tf.math.log(action_logits_selected)\n",
    "            \n",
    "        ratios = tf.math.exp(action_log_selected - tf.stop_gradient(action_log_selected_old))\n",
    "            \n",
    "        eps_clip = 0.2 \n",
    "        surr1 = ratios * advantages\n",
    "        surr2 = tf.clip_by_value(ratios, 1 - eps_clip, 1 + eps_clip) * advantages\n",
    "            \n",
    "        actor_loss = -tf.math.minimum(surr1, surr2)\n",
    "        actor_loss = tf.math.reduce_mean(actor_loss)\n",
    "        actor_loss = tf.cast(actor_loss, 'float32')\n",
    "        #print(\"actor_loss: \", actor_loss)\n",
    "            \n",
    "        critic_loss = mse_loss(values, discounted_r)\n",
    "        total_loss = actor_loss + critic_loss\n",
    "        \n",
    "        return total_loss\n",
    "        \n",
    "    def replay(self, states, actions, rewards, initial_memory_state, initial_carry_state):\n",
    "        # reshape memory to appropriate shape for training\n",
    "        states = np.vstack(states)\n",
    "        \n",
    "        # Compute discounted rewards\n",
    "        discounted_r = self.discount_rewards(rewards)\n",
    "        discounted_r = discounted_r.astype(np.float32)\n",
    "        with tf.GradientTape() as tape:\n",
    "            total_loss = self.get_loss(states, actions, discounted_r, initial_memory_state, initial_carry_state)\n",
    "                 \n",
    "        #print(\"total_loss: \", total_loss)\n",
    "        #print(\"\")\n",
    "            \n",
    "        grads = tape.gradient(total_loss, self.policy.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.policy.trainable_variables))\n",
    " \n",
    "        for a, b in zip(self.policy_old.variables, self.policy.variables):\n",
    "            a.assign(b)  # copies the variables of model_b into model_a\n",
    "        \n",
    "        return total_loss\n",
    "        \n",
    "    def load(self, model_name):\n",
    "        self.ActorCritic = load_model(model_name, compile=False)\n",
    "        #self.Critic = load_model(Critic_name, compile=False)\n",
    "\n",
    "    def save(self):\n",
    "        self.ActorCritic.save(self.model_name)\n",
    "        #self.Critic.save(self.Model_name + '_Critic.h5')\n",
    "\n",
    "    def GetScoreAverage(self, score, episode):\n",
    "        self.scores.append(score)\n",
    "        self.episodes.append(episode)\n",
    "        self.average.append(sum(self.scores[-50:]) / len(self.scores[-50:]))\n",
    "        \n",
    "        return self.average[-1]\n",
    "    \n",
    "    def imshow(self, image, rem_step=0):\n",
    "        cv2.imshow(self.model_name + str(rem_step), image[rem_step,...])\n",
    "        if cv2.waitKey(25) & 0xFF == ord(\"q\"):\n",
    "            cv2.destroyAllWindows()\n",
    "            return\n",
    "\n",
    "    def reset(self, env):\n",
    "        image_memory = np.zeros(self.state_size)\n",
    "        env.reset()\n",
    "        state = env.observations()  \n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def step(self, action, env, image_memory):\n",
    "        #action = random.choice(ACTION_LIST)\n",
    "        reward = env.step(ACTIONS[ACTION_LIST[action]])\n",
    "        if not env.is_running():\n",
    "            print('Environment stopped early')\n",
    "            return 0, 0, True\n",
    "        \n",
    "        next_state = env.observations()\n",
    "        self.render(next_state)\n",
    "        \n",
    "        return next_state, reward, False\n",
    "    \n",
    "    def train(self, n_threads):\n",
    "        self.env.close()\n",
    "        # Instantiate one environment per thread\n",
    "        envs = [deepmind_lab.Lab(self.env_name, ['RGB_INTERLEAVED'], \n",
    "                                 {'fps': '15', 'width': '64', 'height': '64'}) for i in range(n_threads)]\n",
    "        # Create threads\n",
    "        threads = [threading.Thread(\n",
    "                target=self.train_threading,\n",
    "                daemon=True,\n",
    "                args=(self,\n",
    "                    envs[i],\n",
    "                    i)) for i in range(n_threads)]\n",
    "\n",
    "        for t in threads:\n",
    "            time.sleep(2)\n",
    "            t.start()\n",
    "            \n",
    "        for t in threads:\n",
    "            time.sleep(10)\n",
    "            t.join()\n",
    "            \n",
    "    def render(self, obs):\n",
    "        cv2.imshow('obs', obs['RGB_INTERLEAVED'])\n",
    "        cv2.waitKey(1)\n",
    "            \n",
    "    def train_threading(self, agent, env, thread):\n",
    "        total_step = 0\n",
    "        while self.episode < self.EPISODES:\n",
    "            # Reset episode\n",
    "            score, done, SAVING = 0, False, ''\n",
    "            state = self.reset(env)\n",
    "            state = state['RGB_INTERLEAVED'].astype(np.float32)\n",
    "            #print(\"state.shape: \", state.shape)\n",
    "\n",
    "            states, actions, rewards = [], [], []\n",
    "            \n",
    "            memory_state = np.zeros([1,128], dtype=np.float32)\n",
    "            carry_state = np.zeros([1,128], dtype=np.float32)\n",
    "            \n",
    "            initial_memory_state = memory_state\n",
    "            initial_carry_state = carry_state\n",
    "            step = 0\n",
    "            while True:\n",
    "                #print(\"state.shape: \", state.shape)\n",
    "                action, prediction = agent.act_old(np.expand_dims(state, 0), memory_state, carry_state, \n",
    "                                                   training=False)\n",
    "                #print(\"prediction: \", prediction)\n",
    "                memory_state = prediction[2]\n",
    "                carry_state = prediction[3]\n",
    "                #print(\"action: \", action)\n",
    "                next_state, reward, done = self.step(action.numpy(), env, state)\n",
    "                if done == True:\n",
    "                    break\n",
    "                \n",
    "                next_state = next_state['RGB_INTERLEAVED'].astype(np.float32)                    \n",
    "                states.append(np.expand_dims(state, 0))\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "\n",
    "                score += reward\n",
    "                state = next_state\n",
    "                step += 1\n",
    "                #if len(states) == 256:\n",
    "                \n",
    "            print(\"step: \", step)\n",
    "            self.lock.acquire()\n",
    "            self.replay(states, actions, rewards, initial_memory_state, initial_carry_state)\n",
    "            self.lock.release()\n",
    "\n",
    "            states, actions, rewards = [], [], []\n",
    "                    \n",
    "            # Update episode count\n",
    "            with self.lock:\n",
    "                average = self.GetScoreAverage(score, self.episode)\n",
    "                with writer.as_default():\n",
    "                    # other model code would go here\n",
    "                    tf.summary.scalar(\"average\", average, step=self.episode)\n",
    "                    writer.flush()\n",
    "                \n",
    "                # saving best models\n",
    "                if average >= self.max_average:\n",
    "                    self.max_average = average\n",
    "                    #self.save()\n",
    "                    SAVING = \"SAVING\"\n",
    "                else:\n",
    "                    SAVING = \"\"\n",
    "\n",
    "                print(\"episode: {}/{}, thread: {}, score: {}, average: {:.2f} {}\".format(self.episode, self.EPISODES, thread, score, average, SAVING))\n",
    "                if(self.episode < self.EPISODES):\n",
    "                    self.episode += 1\n",
    "\n",
    "        env.close()            \n",
    "\n",
    "    def test(self, Actor_name, Critic_name):\n",
    "        self.load(Actor_name, Critic_name)\n",
    "        for e in range(100):\n",
    "            state = self.reset(self.env)\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = np.argmax(self.Actor.predict(state))\n",
    "                state, reward, done, _ = self.step(action, self.env, state)\n",
    "                score += reward\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, score))\n",
    "                    break\n",
    "\n",
    "        self.env.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env_name = 'ctf_simple'\n",
    "    #env_name = 'Pong-v0'\n",
    "    agent = A3CAgent(env_name)\n",
    "    \n",
    "    #agent.run() # use as A2C\n",
    "    agent.train(n_threads=1) # use as A3C\n",
    "    #agent.test('Models/Pong-v0_A3C_2.5e-05_Actor.h5', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
